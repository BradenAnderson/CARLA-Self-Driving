**Project Overview:** The goal of this project was to explore using reinforcement learning in the setting of autonomous vehicles by creating a self driving agent and training in the CARLA simulator. Unfortunately, due to computational resources adequate modeling and experiementation turned out to be infeasible. A significant amount of work on this project remains, which includes investigating the impact of various engineering decisions on the performance of the learned policy. The following is a short (not comprehensive) list of experiements that should be run:

1. Tune the temperature parameter alpha which balances the agents relative desired for high entropy (explore) and obtainining high rewards (exploit). Ideally the most recent implementation of SAC with the automatic temperature adjustment via an additional network and loss function would be used, but at the very least several experiments should be performed where the alpha value is varied manually (e.g. alpha = 0.3, 0.4, 0.5). 
2. Strategies for training the shared convolutional backbone. Experiment to understand which networks should be allowed to backpropagate their errors to the shared backbone and update its weights (only actor network or both actor and critic networks). 
3. Experiement with the state representation. More advanced experiements could be done such as using multiple cameras or additional sensor types, however at the very least experiements where the state is represented by small stacks of reasonably sized images should be performed. In this context, experimentation could include varying the time delta between images in the stack, number and size of images in the stack, and whether or not the images should be left as RGB or converted to greyscale.
4. Add priority experience replay (instead of the currently utilized randomly selected replay buffer). 

**Note:** Due to a current lack of computational resources to perform any meaningful experimentation this project is not portfolio ready, and will not be posted on any public sites until further work can be done. That said, I still need to thank many of the resources that helped me get as far as I did.


**Resources:** 

Coming into this project, I knew nothing about reinforcement learning, the CARLA simulator for self driving research or running GPU instances in the cloud. These are all things I tried to do in this project, and are part of the longer list of things that I went from knowing ~nothing about to knowing at least a little bit about. This learning would not have been possible without many helpful resources online. I want to give credit here to some of the resources that played the most sigificant roles in helping me get going. See list below (in no specific order):

1. [Machine Learning Phil](https://www.youtube.com/channel/UC58v9cLitc8VaCjrcKyAbrw) has a great video on implementing the original version of Soft Actor Critic (This is the version described in the first SAC paper ("Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", Haarnoja et al.). Although this is not the version of SAC that I tried to implement, his videos and perspectives were still very helpful in understanding how SAC works. Additionally, he had a simple but great implementation of a replay buffer which I also utilized in my project.

2. [OpenAI spinning up libary](https://spinningup.openai.com/en/latest/algorithms/sac.html#soft-actor-critic) has a lot of great info helpful in understanding soft-actor critic, including the steps you need to code out when implementing the algorithm. Warning, if you use this resource in addition to the original papers, note that OpenAI swapped the notation for the network weight vectors. (OpenAI used theta for policy and phi for critic, but the original papers use the opposite convention). 

3. [Deep RL With Python Book](https://www.amazon.com/Deep-Reinforcement-Learning-Python-TensorFlow/dp/1484268083) Out of 10+ books I looked at between Packt, Manning and Oriley publishing websites, this book was the one I found the most useful. The author does a really nice job helping to explain the algorithms and give examples. One part of SAC that I still am fuzzy on is the reparameterization trick used when creating the policy loss, where we change the expectation over the policy parameters to an expectation over random noise (which helps because we don't want to take the gradient of an expectation where the expectation is over the same variable we need to take the gradient with respect to?). The author gives an example of how to accomplish this in code which I utilized in my own project as well.

Note: This book is actually completely free if you have a Seattle Public Library card via the Oriley partnership login, if interested it maybe worth checking if a library near you also allows access to the oriley online library). 

4. [Sentdex from pythonprogramming](https://pythonprogramming.net/introduction-self-driving-autonomous-cars-carla-python/) gives a really nice intro to using the CARLA simulator. (His approach to reinforcement learning is a little too free wheeling in my opinion, but still useful to learn from). This is really nice because the CARLA API is huge, and getting started with it can easily be overwhelming. Having these videos break down some of the basics of getting the enviornment set up was awesome and really helped me get going. Also by being on his website, I also learned about the cloud platform Linode, which I tried and seemed much easier/simpler than GCP just based on first experiences. I am sure I will try to use Linode again in the future. Also, his tutorials showed a "ModifiedTensorboard" class, which subclasses the normal tensorboard class to make the updates more appropriate for reinforcement learning (where we may be calling .fit() a lot more often than normal). The code he provides is for tensorflow 1 and will not work out of the box in tensorflow 2. After working on converting it to tensorflow 2, I found a few different variations of the converted code online which helped me get tensorboard working in my own implementation. I had never used tensorboard in a project before this but found it to be very helpful. It is definitely something worth adding to any project where you need to train neural networks. 

5. [Oliver Sigauds explaination of SAC](https://www.youtube.com/watch?v=_nFXOZpo50U) also gave some great insight into what the SAC algorithm is doing. He explains the balance between sample efficiency and stability (characteristics of two other common families of RL model types) that SAC tries to accomplish, and quickly goes over the details of the papers. Additionally, there is a lot of potential confusion when researching SAC online due to the fact that the algorithm has evolved over time and so three different versions of it exist. Oliver does a nice job clearing this up. 

6. Alexander Amini from MIT gives a great [intro to RL](https://www.youtube.com/watch?v=93M1l_nrhpQ) lecture. Additionally, [his paper](https://www.mit.edu/~amini/pubs/pdf/learning-in-simulation-vista.pdf) does a great job explaining the challenges of going from RL simulations to real world application. The simulator they built, [VISTA](https://www.mit.edu/~amini/vista/) helps bridge this gap. They plan to open source the simulator at some point which I am excited to check out. His explanation regarding the need for photo-realistic simulators is something I used to help motivate my presentation, and it also helped me understand where we are at today in terms of finding practical applications for RL.  



